{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preparing and loading your data\n",
    "This tutorial introduces how SchNetPack stores and loads data.\n",
    "Before we can start training neural networks with SchNetPack, we need to prepare our data.\n",
    "This is because SchNetPack has to stream the reference data from disk during training in order to be able to handle large datasets.\n",
    "Therefore, it is crucial to use data format that allows for fast random read access.\n",
    "We found that the [ASE database format](https://wiki.fysik.dtu.dk/ase/ase/db/db.html) fulfills this criterion perfectly.\n",
    "To further improve the performance, we internally encode properties in binary.\n",
    "However, as long as you only access the ASE database via the provided SchNetPack `ASEAtomsData` class, you don't have to worry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from schnetpack.data import ASEAtomsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predefined datasets\n",
    "SchNetPack supports several benchmark datasets that can be used without preparation.\n",
    "Each one can be accessed using a corresponding class that inherits from `AtomsDataModule` (a specialized PyTorchLightning `DataModule`), which supports automatic download, conversion and partitioning. Here, we show how to use these data sets at the example of the QM9 benchmark.\n",
    "\n",
    "First, we have to import the dataset class and instantiate it. This will automatically download the data to the specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from schnetpack.datasets import QM9\n",
    "from schnetpack.transform import ASENeighborList\n",
    "\n",
    "qm9data = QM9(\n",
    "    './qm9.db', \n",
    "    batch_size=10,\n",
    "    num_train=110000,\n",
    "    num_val=10000,\n",
    "    transforms=[ASENeighborList(cutoff=5.)]\n",
    ")\n",
    "qm9data.prepare_data()\n",
    "qm9data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Neighbors are collected using neighborlists that can be passed to the `AtomsDataModule` as a preprocessing transform. These are applied to the molecules before they are batched in the data loader. We supply different environment providers using a cutoff (e.g., `AseEnvironmentProvider`, `TorchEnvironmentProvider`) that are able to handle larger molecules and periodic boundary conditions.\n",
    "\n",
    "Let's have a closer look at this dataset.\n",
    "We can find out how large it is and which properties it supports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reference calculations: 133885\n",
      "Number of train data: 110000\n",
      "Number of validation data: 10000\n",
      "Number of test data: 13885\n",
      "Available properties:\n",
      "- rotational_constant_A\n",
      "- rotational_constant_B\n",
      "- rotational_constant_C\n",
      "- dipole_moment\n",
      "- isotropic_polarizability\n",
      "- homo\n",
      "- lumo\n",
      "- gap\n",
      "- electronic_spatial_extent\n",
      "- zpve\n",
      "- energy_U0\n",
      "- energy_U\n",
      "- enthalpy_H\n",
      "- free_energy\n",
      "- heat_capacity\n"
     ]
    }
   ],
   "source": [
    "print('Number of reference calculations:', len(qm9data.dataset))\n",
    "print('Number of train data:', len(qm9data.train_dataset))\n",
    "print('Number of validation data:', len(qm9data.val_dataset))\n",
    "print('Number of test data:', len(qm9data.test_dataset))\n",
    "print('Available properties:')\n",
    "\n",
    "for p in qm9data.dataset.available_properties:\n",
    "    print('-', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can load data points  using zero-base indexing. The result is a dictionary containing the geometry and properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties:\n",
      "- _idx : torch.Size([1])\n",
      "- rotational_constant_A : torch.Size([1])\n",
      "- rotational_constant_B : torch.Size([1])\n",
      "- rotational_constant_C : torch.Size([1])\n",
      "- dipole_moment : torch.Size([1])\n",
      "- isotropic_polarizability : torch.Size([1])\n",
      "- homo : torch.Size([1])\n",
      "- lumo : torch.Size([1])\n",
      "- gap : torch.Size([1])\n",
      "- electronic_spatial_extent : torch.Size([1])\n",
      "- zpve : torch.Size([1])\n",
      "- energy_U0 : torch.Size([1])\n",
      "- energy_U : torch.Size([1])\n",
      "- enthalpy_H : torch.Size([1])\n",
      "- free_energy : torch.Size([1])\n",
      "- heat_capacity : torch.Size([1])\n",
      "- _n_atoms : torch.Size([1])\n",
      "- _atomic_numbers : torch.Size([5])\n",
      "- _positions : torch.Size([5, 3])\n",
      "- _cell : torch.Size([1, 3, 3])\n",
      "- _pbc : torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "example = qm9data.dataset[0]\n",
    "print('Properties:')\n",
    "\n",
    "for k, v in example.items():\n",
    "    print('-', k, ':', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that all available properties have been loaded as torch tensors with the given shapes. Keys with an underscore indicate that these names are reserved for internal use. This includes the geometry (`_n_atoms`, `_atomic_numbers`, `_positions`), the index within the dataset (`_idx`) as well as information about neighboring atoms and periodic boundary conditions (`_cell`, `_pbc`). \n",
    "\n",
    "\n",
    "We can iterate the dataset partitions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_idx', 'rotational_constant_A', 'rotational_constant_B', 'rotational_constant_C', 'dipole_moment', 'isotropic_polarizability', 'homo', 'lumo', 'gap', 'electronic_spatial_extent', 'zpve', 'energy_U0', 'energy_U', 'enthalpy_H', 'free_energy', 'heat_capacity', '_n_atoms', '_atomic_numbers', '_positions', '_cell', '_pbc', '_idx_i_local', '_idx_j_local', '_offsets', '_idx_m', '_idx_i', '_idx_j'])\n"
     ]
    }
   ],
   "source": [
    "for batch in qm9data.val_dataloader():\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that additional keys have been added by the neighborlist transform defined above. These are the relative positions (`_Rij`) and neighbor indices (`_idx_i`, `_idx_j`). Since differrent systems can have different numbers of atoms, we don't use separate dimensions for systems and atoms (i.e. shape [n_systems, n_atoms, ...]), but store the atoms of all systems in a single dimension (i.e. shape [n_all_atoms, ...]). Therefore, we additionally need to store the indices of the corresponding system for each atom in a batch (`idx_m`). This avoids the padding and masking that was required in previous versions of SchNetPack. The indices look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System index: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n",
      "Center atom index: tensor([  0,   0,   0,  ..., 155, 155, 155])\n",
      "Neighbor atom index: tensor([  1,  14,  12,  ..., 141, 146, 154])\n"
     ]
    }
   ],
   "source": [
    "print('System index:', batch['_idx_m'])\n",
    "print('Center atom index:', batch['_idx_i'])\n",
    "print('Neighbor atom index:', batch['_idx_j'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "All property names are pre-defined as class-variable for convenient access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total energy at 0K: tensor([-450.2210, -437.8808, -333.4494, -403.1605, -387.1264, -472.7010,\n",
      "        -385.8485, -470.0007, -434.0736, -453.9216], dtype=torch.float64)\n",
      "HOMO: tensor([-0.2451, -0.2660, -0.2690, -0.2507, -0.2318, -0.2676, -0.2343, -0.2327,\n",
      "        -0.2768, -0.2193], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print('Total energy at 0K:', batch[QM9.U0])\n",
    "print('HOMO:', batch[QM9.homo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparing your own data\n",
    "In the following we will create an ASE database from our own data.\n",
    "For this tutorial, we will use a dataset containing a molecular dynamics (MD) trajectory of ethanol, which can be downloaded [here](http://quantum-machine.org/gdml/data/xyz/ethanol_dft.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('./uracil_dft.npz'):\n",
    "    !wget http://quantum-machine.org/gdml/data/npz/uracil_dft.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data set is in Numpy format. \n",
    "In the following, we show how this data can be parsed and converted for use in SchNetPack, so that you apply this to any other data format.\n",
    "\n",
    "First, we need to parse our data. For this we use the IO functionality supplied by ASE.\n",
    "In order to create a SchNetPack DB, we require a **list of ASE `Atoms` objects** as well as a corresponding **list of dictionaries** `[{property_name1: property1_molecule1}, {property_name1: property1_molecule2}, ...]` containing the mapping from property names to values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties: {'energy': array([-260120.11049893]), 'forces': array([[ 8.58762909e-01,  7.93783439e+00, -6.10513446e-01],\n",
      "       [-1.59587602e+01, -1.38921103e+01,  1.46274031e+00],\n",
      "       [ 2.38826967e+01,  6.32292299e+01, -5.38152591e+00],\n",
      "       [-2.39499309e+01, -5.43241035e+00,  1.04116149e+00],\n",
      "       [ 6.26392709e+01, -6.78667364e+01,  3.52832279e+00],\n",
      "       [-2.38302859e+01,  8.54756612e+00, -3.88276505e-02],\n",
      "       [ 2.46907632e+00,  3.17072923e-01, -9.76762146e-02],\n",
      "       [ 1.47304240e+01,  1.43791066e+01, -1.46191820e+00],\n",
      "       [-6.34016421e+00,  2.75814146e+00, -5.13516521e-02],\n",
      "       [-1.13403416e+01, -1.98585305e+01,  1.77899100e+00],\n",
      "       [-1.58345920e+01,  1.17016462e+01, -4.86097667e-01],\n",
      "       [-7.32615606e+00, -1.82081008e+00,  3.16695146e-01]])}\n"
     ]
    }
   ],
   "source": [
    "from ase import Atoms\n",
    "import numpy as np\n",
    "\n",
    "# load atoms from npz file. Here, we only parse the first 10 molecules\n",
    "data = np.load('./uracil_dft.npz')\n",
    "\n",
    "numbers = data[\"z\"]\n",
    "atoms_list = []\n",
    "property_list = []\n",
    "for positions, energies, forces in zip(data[\"R\"], data[\"E\"], data[\"F\"]):\n",
    "    ats = Atoms(positions=positions, numbers=numbers)\n",
    "    properties = {'energy': energies, 'forces': forces}\n",
    "    property_list.append(properties)\n",
    "    atoms_list.append(ats)\n",
    "    \n",
    "print('Properties:', property_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once we have our data in this format, it is straightforward to create a new SchNetPack DB and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%rm './new_dataset.db'\n",
    "new_dataset = ASEAtomsData.create(\n",
    "    './new_dataset.db', \n",
    "    distance_unit='Ang',\n",
    "    property_unit_dict={'energy':'kcal/mol', 'forces':'kcal/mol/Ang'}\n",
    ")\n",
    "new_dataset.add_systems(property_list, atoms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can have a look at the data in the same way we did before for QM9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reference calculations: 133770\n",
      "Available properties:\n",
      "- energy\n",
      "- forces\n",
      "\n",
      "Properties of molecule with id 0:\n",
      "- _idx : torch.Size([1])\n",
      "- energy : torch.Size([1])\n",
      "- forces : torch.Size([12, 3])\n",
      "- _n_atoms : torch.Size([1])\n",
      "- _atomic_numbers : torch.Size([12])\n",
      "- _positions : torch.Size([12, 3])\n",
      "- _cell : torch.Size([1, 3, 3])\n",
      "- _pbc : torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print('Number of reference calculations:', len(new_dataset))\n",
    "print('Available properties:')\n",
    "\n",
    "for p in new_dataset.available_properties:\n",
    "    print('-', p)\n",
    "print()    \n",
    "\n",
    "example = new_dataset[0]\n",
    "print('Properties of molecule with id 0:')\n",
    "\n",
    "for k, v in example.items():\n",
    "    print('-', k, ':', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The same way, we can store multiple properties, including atomic properties such as forces, or tensorial properties such as polarizability tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datasets with Periodic Boundary Conditions\n",
    "SchNetPack can also be used for materials structures or other structures that require periodic boundary conditions (PBC). PBC allow to effectively reduce the number of simulated particles to only a fraction of the actual system's size. This is achieved by considering a relatively small simulation box, which is periodically repeated at its boundaries. In most cases, the resulting simulated periodic structure is a good approximation of the system under consideration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load raw data\n",
    "data = np.load(\"nacl_snippet.npz\")\n",
    "\n",
    "# collect atoms and properties\n",
    "numbers = data[\"z\"]\n",
    "atoms_list = []\n",
    "property_list = []\n",
    "for positions, cell, energies, forces, stress in zip(\n",
    "        data[\"R\"], data[\"L\"], data[\"E\"], data[\"F\"], data[\"S\"],\n",
    "):\n",
    "    # unsqueeze cell and stress\n",
    "    cell, stress = cell[None, :], stress[None, :]\n",
    "    # build atoms and collect data\n",
    "    ats = Atoms(positions=positions, numbers=numbers, cell=cell, pbc=True)\n",
    "    properties = {'energy': energies, 'forces': forces, 'stress': stress}\n",
    "    property_list.append(properties)\n",
    "    atoms_list.append(ats)\n",
    "\n",
    "print('Properties:', property_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Custom Datasets\n",
    "If we want to share datasets or reuse a dataset frequently it makes sense to define a custom dataset class for this dataset. In the custom dataset class we can define all steps from downloading the dataset to the creation of the ready to use ase database. This allows to automate the data preparation on other machines without the need of copying database files manually.\n",
    "For simplicity we will stick with the uracil dataset from the example above and create a new dataset class called `MyUracil`. In order to create a custom dataset class we need to derive the new class from `schnetpack.data.AtomsDataModule` and add the methods `prepare_data()` and `_download()`. Furthermore we recommend to add dataset specific information (e.g. available properties, atomrefs, ...) as class variables. This can prevent typo bugs and displays the keys of available properties. The emtpy dataset class is sketched below. During the course of this section we will fill the empty methods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "import wget\n",
    "from schnetpack.data import AtomsDataModule, AtomsDataModuleError\n",
    "from schnetpack.data import create_dataset, load_dataset\n",
    "\n",
    "class MyUracil(AtomsDataModule):\n",
    "    \"\"\"\n",
    "    Tutorial dataset for uracil.\n",
    "\n",
    "    Args:\n",
    "        datapath: path to dataset\n",
    "        batch_size: (train) batch size\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    energy = \"energy\"\n",
    "    forces = \"forces\"\n",
    "    atomrefs = {}\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            datapath: str,\n",
    "            batch_size: int,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super(MyUracil, self).__init__(\n",
    "            datapath=datapath,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def _download(self, dataset):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our first method called `create_data` should check if the dataset is available at the provided location. If no data is present we create an empty dataset and fill it with downloaded data. We do this by using the `_download()` method. Furthermore it can make sense add some checks here. This helps to verify the provided data located at `datapath`. The implementation of `prepare_data()` can be done straight forward:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    def prepare_data(self):\n",
    "        # download data if not present at location\n",
    "        if not os.path.exists(self.datapath):\n",
    "            # create empty dataset\n",
    "            property_unit_dict = {\n",
    "                MyUracil.energy: \"eV\",\n",
    "                MyUracil.forces: \"eV/Ang\",\n",
    "            }\n",
    "            dataset = create_dataset(\n",
    "                datapath=self.datapath,\n",
    "                format=self.format,\n",
    "                distance_unit=\"Ang\",\n",
    "                property_unit_dict=property_unit_dict,\n",
    "                atomrefs=MyUracil.atomrefs,\n",
    "            )\n",
    "            # download data and fill dataset\n",
    "            self._download_data(dataset)\n",
    "\n",
    "        # load dataset if data is available\n",
    "        else:\n",
    "            dataset = load_dataset(self.datapath, self.format)\n",
    "\n",
    "         # checks\n",
    "        if len(dataset) == 0:\n",
    "            raise AtomsDataModuleError(\n",
    "                f\"The dataset located at {self.datapath} is empty.\"\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we defined the frame for our custom dataset class, we only need to fill the `_download` method. This method should handle the full workflow including the download of raw data, the parsing of the data file(s) and the creation of data format, that is compatible with `schnetpack`. Since the parsing of our example data has already been explained in the previous section, we can more or less just copy-paste the parsing to our new method:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    def _download(self, dataset):\n",
    "        # load raw data\n",
    "        tmp_dir = tempfile.mkdtemp()\n",
    "        _ = wget.download(\n",
    "            \"http://quantum-machine.org/gdml/data/npz/uracil_dft.npz\",\n",
    "            os.path.join(tmp_dir, f\"uracil_dft.npz\"),\n",
    "        )\n",
    "        data = np.load(tmp_dir, f\"uracil_dft.npz\")\n",
    "        shutil.rmtree(tmp_dir)\n",
    "\n",
    "        # parse atoms and properties\n",
    "        atoms_list = []\n",
    "        property_list = []\n",
    "        numbers = data[\"z\"]\n",
    "        for positions, energies, forces in zip(data[\"R\"], data[\"E\"], data[\"F\"]):\n",
    "            ats = Atoms(positions=positions, numbers=numbers)\n",
    "            properties = {'energy': energies, 'forces': forces}\n",
    "            property_list.append(properties)\n",
    "            atoms_list.append(ats)\n",
    "\n",
    "        # write data to dataset\n",
    "        dataset.add_systems(property_list, atoms_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get the working dataset class by adding the two methods to the emtpy dataset class:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyUracil(AtomsDataModule):\n",
    "    \"\"\"\n",
    "    Tutorial dataset for uracil.\n",
    "\n",
    "    Args:\n",
    "        datapath: path to dataset\n",
    "        batch_size: (train) batch size\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    energy = \"energy\"\n",
    "    forces = \"forces\"\n",
    "    atomrefs = {}\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            datapath: str,\n",
    "            batch_size: int,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super(MyUracil, self).__init__(\n",
    "            datapath=datapath,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download data if not present at location\n",
    "        if not os.path.exists(self.datapath):\n",
    "            # create empty dataset\n",
    "            property_unit_dict = {\n",
    "                MyUracil.energy: \"eV\",\n",
    "                MyUracil.forces: \"eV/Ang\",\n",
    "            }\n",
    "            dataset = create_dataset(\n",
    "                datapath=self.datapath,\n",
    "                format=self.format,\n",
    "                distance_unit=\"Ang\",\n",
    "                property_unit_dict=property_unit_dict,\n",
    "                atomrefs=MyUracil.atomrefs,\n",
    "            )\n",
    "            # download data and fill dataset\n",
    "            self._download_data(dataset)\n",
    "\n",
    "        # load dataset if data is available\n",
    "        else:\n",
    "            dataset = load_dataset(self.datapath, self.format)\n",
    "\n",
    "         # checks\n",
    "        if len(dataset) == 0:\n",
    "            raise AtomsDataModuleError(\n",
    "                f\"The dataset located at {self.datapath} is empty.\"\n",
    "            )\n",
    "\n",
    "    def _download_data(self, dataset):\n",
    "        # load raw data\n",
    "        tmp_dir = tempfile.mkdtemp()\n",
    "        _ = wget.download(\n",
    "            \"http://quantum-machine.org/gdml/data/npz/uracil_dft.npz\",\n",
    "            os.path.join(tmp_dir, f\"uracil_dft.npz\"),\n",
    "        )\n",
    "        data = np.load(tmp_dir, f\"uracil_dft.npz\")\n",
    "        shutil.rmtree(tmp_dir)\n",
    "\n",
    "        # parse atoms and properties\n",
    "        atoms_list = []\n",
    "        property_list = []\n",
    "        numbers = data[\"z\"]\n",
    "        for positions, energies, forces in zip(data[\"R\"], data[\"E\"], data[\"F\"]):\n",
    "            ats = Atoms(positions=positions, numbers=numbers)\n",
    "            properties = {'energy': energies, 'forces': forces}\n",
    "            property_list.append(properties)\n",
    "            atoms_list.append(ats)\n",
    "\n",
    "        # write data to dataset\n",
    "        dataset.add_systems(property_list, atoms_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have created our own dataset, which can be used in the same way as the predefined datasets. Let`s see if everything worked out well:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_uracil = MyUracil(datapath=\"my_uracil.db\", batch_size=10)\n",
    "my_uracil.prepare_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This looks good.\n",
    "In the following tutorials, we will describe how these datasets can be used to train neural networks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spkdev] *",
   "language": "python",
   "name": "conda-env-spkdev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}